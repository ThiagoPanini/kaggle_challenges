{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libs-and-Functions\" data-toc-modified-id=\"Libs-and-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libs and Functions</a></span></li><li><span><a href=\"#Data-Understanding\" data-toc-modified-id=\"Data-Understanding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Understanding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Null-Data\" data-toc-modified-id=\"Null-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Null Data</a></span></li><li><span><a href=\"#Data-Types\" data-toc-modified-id=\"Data-Types-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data Types</a></span></li></ul></li><li><span><a href=\"#Graphical-Exploration\" data-toc-modified-id=\"Graphical-Exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Graphical Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evolution-of-E-Commerce-Overtime\" data-toc-modified-id=\"Evolution-of-E-Commerce-Overtime-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Evolution of E-Commerce Overtime</a></span></li><li><span><a href=\"#Online-Sales-by-Region\" data-toc-modified-id=\"Online-Sales-by-Region-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Online Sales by Region</a></span></li><li><span><a href=\"#Payment-Type-on-E-Commerce\" data-toc-modified-id=\"Payment-Type-on-E-Commerce-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Payment Type on E-Commerce</a></span></li><li><span><a href=\"#Review-Score\" data-toc-modified-id=\"Review-Score-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Review Score</a></span></li></ul></li><li><span><a href=\"#Natural-Language-Processing\" data-toc-modified-id=\"Natural-Language-Processing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Natural Language Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#RegEx\" data-toc-modified-id=\"RegEx-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>RegEx</a></span><ul class=\"toc-item\"><li><span><a href=\"#Break-Line-and-Carriage-Return\" data-toc-modified-id=\"Break-Line-and-Carriage-Return-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Break Line and Carriage Return</a></span></li><li><span><a href=\"#Sites-and-Hyperlinks\" data-toc-modified-id=\"Sites-and-Hyperlinks-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Sites and Hyperlinks</a></span></li><li><span><a href=\"#Numbers\" data-toc-modified-id=\"Numbers-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Numbers</a></span></li><li><span><a href=\"#Special-Characteres\" data-toc-modified-id=\"Special-Characteres-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Special Characteres</a></span></li><li><span><a href=\"#Additional-Whitespaces\" data-toc-modified-id=\"Additional-Whitespaces-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>Additional Whitespaces</a></span></li></ul></li><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Stopwords</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Sentiment-Label\" data-toc-modified-id=\"Sentiment-Label-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Sentiment Label</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Pre-Processing-Pipeline\" data-toc-modified-id=\"Pre-Processing-Pipeline-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Pre Processing Pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-a-Pipeline\" data-toc-modified-id=\"Building-a-Pipeline-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>Building a Pipeline</a></span></li><li><span><a href=\"#Applying-a-Pipeline\" data-toc-modified-id=\"Applying-a-Pipeline-4.6.2\"><span class=\"toc-item-num\">4.6.2&nbsp;&nbsp;</span>Applying a Pipeline</a></span></li></ul></li><li><span><a href=\"#Training-Models\" data-toc-modified-id=\"Training-Models-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Training Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Testing</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to propose an analytical view of e-commerce relationship in Brazil. For this we will first go trough an exploratory data analysis using graphical tools to create self explanatory plots for better understanding what is behind braziian online purchasing. Finally we will look at customers reviews and implement **_Sentimental Analysis_** to make a text classification using **_Natural Language Process_** tools.\n",
    "\n",
    "English is not my mother language. Sorry for any mistakes. If you like this kernel, **please upvote!**\n",
    "\n",
    "Let's get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:27.277665Z",
     "start_time": "2019-08-27T20:10:25.712083Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import missingno as msno\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import string\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import folium\n",
    "from folium.plugins import Fullscreen\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T21:04:29.649438Z",
     "start_time": "2019-08-27T21:04:29.5983Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Defining functions\n",
    "def format_spines(ax, right_border=True):\n",
    "    \"\"\"\n",
    "    This function is responsible for format axis from graphs\n",
    "    \n",
    "    Input:\n",
    "        ax: matplotlib axis\n",
    "        right_border: boolean flag for plot the right border of graphs\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    ax.spines['bottom'].set_color('#CCCCCC')\n",
    "    ax.spines['left'].set_color('#CCCCCC')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    if right_border:\n",
    "        ax.spines['right'].set_color('#CCCCCC')\n",
    "    else:\n",
    "        ax.spines['right'].set_color('#FFFFFF')\n",
    "    ax.patch.set_facecolor('#FFFFFF')\n",
    "    \n",
    "def bar_plot(x, y, df, ax, colors='Blues_d', hue=False, value=False):\n",
    "    \"\"\"\n",
    "    This function plots, and customize a bar chart\n",
    "\n",
    "    Input:\n",
    "        x: feature to be plotted on x axis\n",
    "        y: feature to be plotted on y axis\n",
    "        df: DataFrame object with features used on x and y\n",
    "        colors: standard palette is \"Blues_d\"\n",
    "        hue: separation value, standard is False\n",
    "        value: flag for defining if the data labels on bars will be the value (True) or percentual (False)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Counting numerical feature (x or y)\n",
    "    try:\n",
    "        ncount = sum(df[y])\n",
    "    except:\n",
    "        ncount = sum(df[x])\n",
    "    \n",
    "    # Plotting\n",
    "    if hue != False:\n",
    "        ax = sns.barplot(x=x, y=y, data=df, palette=colors, hue=hue, ax=ax, ci=None)\n",
    "    else:\n",
    "        ax = sns.barplot(x=x, y=y, data=df, palette=colors, ax=ax, ci=None)\n",
    "\n",
    "    # Customizing data labels (values of percents)\n",
    "    for p in ax.patches:\n",
    "        xp=p.get_bbox().get_points()[:,0]\n",
    "        yp=p.get_bbox().get_points()[1,1]\n",
    "        if value:\n",
    "            ax.annotate('{:.2f}k'.format(yp/1000), (xp.mean(), yp), \n",
    "                    ha='center', va='bottom')\n",
    "        else:\n",
    "            ax.annotate('{:.1f}%'.format(100.*yp/ncount), (xp.mean(), yp), \n",
    "                    ha='center', va='bottom')\n",
    "            \n",
    "def add_series_working_days(series_name, df, date_col1, date_col2):\n",
    "    \"\"\"\n",
    "    This function is used for calculating working days between two dates\n",
    "    as an additional column on a DataFrame\n",
    "    \n",
    "    Input:\n",
    "        series_names: name of the new series created with working days calculated\n",
    "        df: DataFrame object with the dates\n",
    "        date_col1: date column 1\n",
    "        date_col2: date column 2\n",
    "        \n",
    "    Returns:\n",
    "        df_return: DataFrame with working days columns\n",
    "    \"\"\"\n",
    "    # Creating a list with the difference between dates\n",
    "    time_list = []\n",
    "    idx = 0\n",
    "    second_date_series = df[date_col2].values.astype('datetime64[D]')\n",
    "    for date in df[date_col1].values.astype('datetime64[D]'):\n",
    "        second_date = second_date_series[idx]\n",
    "        try:\n",
    "            workdays = np.busday_count(date, second_date)\n",
    "        except:\n",
    "            workdays = np.NaN\n",
    "        time_list.append(workdays)\n",
    "        idx += 1\n",
    "    \n",
    "    # Adding column in a DataFrame object\n",
    "    df_return = df.copy()\n",
    "    df_return[series_name] = pd.Series(time_list)\n",
    "    df_return.dropna(inplace=True)\n",
    "    \n",
    "    return df_return\n",
    "\n",
    "def communicate_params(freight, deliv, est):\n",
    "    \"\"\"\n",
    "    Additional function create for helping on graphical exploration (state dashboard)\n",
    "    \n",
    "    Input:\n",
    "        freight: freight value to be plotted as a text\n",
    "        deliv: delivery time to be plotted as a text\n",
    "        estimative: differente between delivery time and estimative time to be plotted as a text\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"  \n",
    "    # Comunicando o frete\n",
    "    axs[0, 0].text(0.10, 0.30, f'R${freight:.2f}', fontsize=45)\n",
    "    axs[0, 0].text(0.07, 0.12, 'is the mean value of freight paid', fontsize=12)\n",
    "    axs[0, 0].text(0.25, 0.00, 'for online shopping', fontsize=12)\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "    # Comunicando tempo médio de entrega\n",
    "    axs[0, 1].text(0.13, 0.30, f'{deliv:.2f}', fontsize=45)\n",
    "    axs[0, 1].text(0.62, 0.30, 'working days', fontsize=12)\n",
    "    axs[0, 1].text(0.07, 0.12, 'is the average delay for delivery', fontsize=12)\n",
    "    axs[0, 1].text(0.23, 0.00, 'for online shopping', fontsize=12)\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    # Comunicando diferença entre tempo de entrega e tempo estimado\n",
    "    axs[0, 2].text(0.18, 0.30, f'{est:.2f}', fontsize=45)\n",
    "    axs[0, 2].text(0.58, 0.30, 'working days', fontsize=12)\n",
    "    axs[0, 2].text(0.07, 0.12, 'is the difference between estimated', fontsize=12)\n",
    "    axs[0, 2].text(0.23, 0.00, 'date and delivery date', fontsize=12)\n",
    "    axs[0, 2].axis('off') \n",
    "    \n",
    "def plot_param(df, col, title, xlim, n_row, n_col, y='customer_state', div_xlim=0, \n",
    "               one_axis=False, xlabel=[], ylabel='State'):\n",
    "    \"\"\"\n",
    "    This function is used for plotting a comparative study on 5 better and 5 worst according to the topic\n",
    "    \n",
    "    Input:\n",
    "        df: DataFrame object with the data\n",
    "        col: column to be studied\n",
    "        title: title\n",
    "        xlim: xlim\n",
    "        n_row: line index where the graph will be plotted\n",
    "        n_col: column index where the graph will be plotted\n",
    "    \"\"\"\n",
    "    # Axis definition\n",
    "    if one_axis:\n",
    "        ax_top = axs[n_col]\n",
    "        ax_last = axs[n_col+1]\n",
    "    else:\n",
    "        ax_top = axs[n_row, n_col]\n",
    "        ax_last = axs[n_row+1, n_col]\n",
    "    \n",
    "    # First step: Top 5\n",
    "    df.sort_values(by=col, ascending=False, inplace=True)\n",
    "    top5 = df.iloc[:5, :]\n",
    "    sns.barplot(x=col, y=y, data=top5, ci=None, palette='Blues_d', ax=ax_top)\n",
    "    format_spines(ax_top, right_border=False)\n",
    "    ax_top.set_title(title)\n",
    "    ax_top.set_xlim(0, xlim)\n",
    "    ax_top.set_xlabel(xlabel)\n",
    "    if n_col > 0:\n",
    "        ax_top.set_ylabel('')\n",
    "    else:\n",
    "        ax_top.set_ylabel(ylabel)\n",
    "    \n",
    "    # Second step: Last 5\n",
    "    last5 = df.iloc[-5:, :]\n",
    "    sns.barplot(x=col, y=y, data=last5, ci=None, palette='Blues_d', ax=ax_last)\n",
    "    format_spines(ax_last, right_border=False)\n",
    "    ax_last.set_title(title.replace('Highest', 'Lowest'))\n",
    "    if div_xlim > 0:\n",
    "        ax_last.set_xlim(0, xlim/div_xlim)\n",
    "    else:\n",
    "        ax_last.set_xlim(0, xlim)\n",
    "    ax_last.set_xlabel(xlabel)\n",
    "    if n_col > 0:\n",
    "        ax_last.set_ylabel('')\n",
    "    else:\n",
    "        ax_last.set_ylabel(ylabel)\n",
    "        \n",
    "def donut_plot(col, ax, df, labels, text='', flag_ruido = 0,\n",
    "               colors=['navy', 'lightsteelblue', 'lightgreen', 'crimson', '']):\n",
    "    \"\"\"\n",
    "    This function plots a customized donut plot\n",
    "    \n",
    "    Input:\n",
    "        col: coluna a ser analisada e plotada no gráfico de rosca\n",
    "        ax: matplotlib axis\n",
    "        df: DataFrame with data\n",
    "        labels: list of labels to be plotted\n",
    "        text: text to be plotted on the center of the donut\n",
    "        flag_ruido: thie parameter indicates the number of labels to be filtered from plot\n",
    "        colors: list of colors (4 colors from default)\n",
    "    \"\"\"\n",
    "    flag_ruido = flag_ruido * -1\n",
    "    if flag_ruido < 0:\n",
    "        sizes = df[col].value_counts().values[:flag_ruido]\n",
    "        labels = labels[:flag_ruido]\n",
    "    else:\n",
    "        sizes = df[col].value_counts().values\n",
    "    center_circle = plt.Circle((0,0), 0.80, color='white')\n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%')\n",
    "    ax.add_artist(center_circle)\n",
    "    kwargs = dict(size=20, fontweight='bold', va='center')\n",
    "    ax.text(0, 0, text, ha='center', **kwargs)\n",
    "    \n",
    "def text_process(c):\n",
    "    \"\"\"\n",
    "    Function responsible for removing punctuation and stopwords from reviews\n",
    "    \n",
    "    Input:\n",
    "        c: customer review\n",
    "    \n",
    "    Output:\n",
    "        reviews without punctuation and stopwords\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    nopunc = [char for char in c if char not in string.punctuation]\n",
    "\n",
    "    # Join string again\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    return [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('portuguese')]\n",
    "\n",
    "def stem_processing(c):\n",
    "    \"\"\"\n",
    "    Function repsonsible for apply stemming on reviews\n",
    "    \n",
    "    Input:\n",
    "        c: customer review\n",
    "        \n",
    "    Output:\n",
    "        review after stemming\n",
    "    \"\"\"\n",
    "    \n",
    "    stemmer = RSLPStemmer()\n",
    "    return list(map(lambda x: stemmer.stem(x), [word for word in c.split()]))\n",
    "\n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Function responsible for creating an empty DataFrame object with some of classification metrics\n",
    "    \n",
    "    Returns:\n",
    "        empty DataFrame object\n",
    "    \"\"\"\n",
    "    attributes = ['acc', 'prec', 'rec', 'f1', 'total_time']\n",
    "    model_performance = pd.DataFrame({})\n",
    "    for col in attributes:\n",
    "        model_performance[col] = []\n",
    "        \n",
    "    return model_performance\n",
    "\n",
    "def model_analysis(model, X, y, X_test, y_test, df_performance, cv=5, train=True):\n",
    "    \"\"\"\n",
    "    Function responsible for evaluate a classification model and save the results on a DataFrame object\n",
    "    \n",
    "    Input:\n",
    "        model: model to be used on evaluation\n",
    "        X, y, X_test, y_test: train and test data (with target labels)\n",
    "        df_performance: empty DataFrame (generated by create_dataset() function)\n",
    "        cv: cross validation k folds\n",
    "\n",
    "    Returns:\n",
    "        a DataFrame object with classification metrics selected\n",
    "    \"\"\"\n",
    "    # Accuracy, precision, recall and f1_score on training set using cv\n",
    "    t0_cv = time.time()\n",
    "    acc = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
    "    prec = cross_val_score(model, X, y, cv=cv, scoring='precision').mean()\n",
    "    rec = cross_val_score(model, X, y, cv=cv, scoring='recall').mean()\n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1').mean()\n",
    "    # Time spent on cross_validation prediction\n",
    "    t1_cv = time.time()\n",
    "    delta_time_cv = t1_cv-t0_cv\n",
    "    \n",
    "    # Evaluation using the test set\n",
    "    t0_test = time.time()\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    prec_test = precision_score(y_test, y_pred_test)\n",
    "    rec_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    y_scores_test = model.predict_proba(X_test)[:, 1]\n",
    "    # Time spent on test prediction\n",
    "    t1_test = time.time()\n",
    "    delta_time_test = t1_test-t0_test\n",
    "\n",
    "    # Saving on dataframe\n",
    "    performances = {}\n",
    "    performances['acc'] = round(acc, 4)\n",
    "    performances['prec'] = round(prec, 4)\n",
    "    performances['rec'] = round(rec, 4)\n",
    "    performances['f1'] = round(f1, 4)\n",
    "    performances['total_time'] = round(delta_time_cv, 3)        \n",
    "    df_performance = df_performance.append(performances, ignore_index=True)\n",
    "    \n",
    "    test_performances = {}\n",
    "    test_performances['acc'] = round(acc_test, 4)\n",
    "    test_performances['prec'] = round(prec_test, 4)\n",
    "    test_performances['rec'] = round(rec_test, 4)\n",
    "    test_performances['f1'] = round(f1_test, 4)\n",
    "    test_performances['total_time'] = round(delta_time_test, 3)        \n",
    "    df_performance = df_performance.append(test_performances, ignore_index=True)\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "    df_performance.index = [model_name+' cv', model_name+' test']\n",
    "    \n",
    "    return df_performance\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a customized confusion matrix\n",
    "    \n",
    "    Input:\n",
    "        cm: confusion matrix generated from sklearn's method confusion_matrix(set, predictions)\n",
    "        classes: target labels to be plotted\n",
    "        title: title\n",
    "        cmap: matrix color\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Plot configuration\n",
    "    thresh = cm.max() / 1.2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def style_function(feature):\n",
    "    \"\"\"\n",
    "    Customize maps\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'fillColor': '#ffaf00',\n",
    "        'color': 'grey',\n",
    "        'weight': 1.5,\n",
    "        'dashArray': '5, 5'\n",
    "    }\n",
    "\n",
    "def highlight_function(feature):\n",
    "    \"\"\"\n",
    "    Customize maps\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'fillColor': '#ffaf00',\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'dashArray': '5, 5'\n",
    "    }\n",
    "\n",
    "def plot_sentimento(model, frase):\n",
    "    \"\"\"\n",
    "    This function receives a text example and uses a classification model to predict\n",
    "    the sentiment label and communicate the result in a graphic plot\n",
    "    \n",
    "    Input:\n",
    "        model: classification model\n",
    "        frase: pre processed text\n",
    "    \n",
    "    Output:\n",
    "        result plotted on matplotlib\n",
    "    \"\"\"\n",
    "    # Predicting\n",
    "    pred = model.predict(frase)\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    if pred[0] == 1:\n",
    "        text = 'Positive'\n",
    "        proba = 100 * round(model.predict_proba(frase)[0][1], 4)\n",
    "        color = 'seagreen'\n",
    "    else:\n",
    "        text = 'Negative'\n",
    "        proba = 100 * round(model.predict_proba(frase)[0][0], 4)\n",
    "        color = 'crimson'\n",
    "    ax.text(0.5, 0.5, text, fontsize=50, ha='center', color=color)\n",
    "    ax.text(0.5, 0.20, str(proba) + '%', fontsize=14, ha='center')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Sentiment', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following datasets:\n",
    "\n",
    "    olist_customers_dataset.csv\n",
    "    olist_geolocation_dataset.csv\n",
    "    olist_orders_dataset.csv\n",
    "    olist_order_items_dataset.csv\n",
    "    olist_order_payments_dataset.csv\n",
    "    olist_order_reviews_dataset.csv\n",
    "    olist_products_dataset.csv\n",
    "    olist_sellers_dataset.csv\n",
    "    product_category_name_translation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between datasets is provided by foreign keys in each one of the sets. It requires specific methods like join, concat and others for joining the sets and making an analysis on them. For this first moment, let's just read the files and see a little bit more of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:29.7493Z",
     "start_time": "2019-08-27T20:10:27.327161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading files\n",
    "olist_customer = pd.read_csv('../input/olist_customers_dataset.csv')\n",
    "olist_geolocation = pd.read_csv('../input/olist_geolocation_dataset.csv')\n",
    "olist_orders = pd.read_csv('../input/olist_orders_dataset.csv')\n",
    "olist_order_items = pd.read_csv('../input/olist_order_items_dataset.csv')\n",
    "olist_order_payments = pd.read_csv('../input/olist_order_payments_dataset.csv')\n",
    "olist_order_reviews = pd.read_csv('../input/olist_order_reviews_dataset.csv')\n",
    "olist_products = pd.read_csv('../input/olist_products_dataset.csv')\n",
    "olist_sellers = pd.read_csv('../input/olist_sellers_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:29.762687Z",
     "start_time": "2019-08-27T20:10:29.75097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of what is on the orders dataset\n",
    "olist_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:29.772087Z",
     "start_time": "2019-08-27T20:10:29.764803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Columns on each set\n",
    "dataframes = {\n",
    "    'Customers': olist_customer,\n",
    "    'Geolocation': olist_geolocation,\n",
    "    'Orders': olist_orders,\n",
    "    'Items': olist_order_items,\n",
    "    'Payments': olist_order_payments,\n",
    "    'Reviews': olist_order_reviews,\n",
    "    'Products': olist_products,\n",
    "    'Sellers': olist_sellers\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f'{name}: {len(df.columns)} columns')\n",
    "    print(f'{list(df.columns)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:29.780167Z",
     "start_time": "2019-08-27T20:10:29.774157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Foreign keys\n",
    "for name, df in dataframes.items():\n",
    "    chaves = [col for col in df.columns if '_id' in col or 'code' in col]\n",
    "    print(f'{name}: {len(chaves)} PKs or FKs')\n",
    "    print(f'{chaves}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bunch of datasets that can be connected using `id` columns. Let's see if we have null data in anyone of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:30.136663Z",
     "start_time": "2019-08-27T20:10:29.782202Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f'{name:<12}- {df.isnull().any().any()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have null data in `orders`, `reviews` and `products` datasets. How many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:30.542504Z",
     "start_time": "2019-08-27T20:10:30.139753Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    if df.isnull().any().any():\n",
    "        print(f'Dataset: {name}\\n')\n",
    "        print(f'{df.isnull().sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Null data in **Orders** dataset are from:\n",
    "\n",
    "    - Orders without approval date;\n",
    "    - Orders that weren't delivered to the carrier;\n",
    "    - Orders that weren't delivered to the customer.\n",
    "\n",
    "\n",
    "* Null data in **Reviews** dataset are from:\n",
    "\n",
    "    - Orders without comments;\n",
    "    - Comments without title;\n",
    "  \n",
    "  \n",
    "* Null data in **Products** dataset are from:\n",
    "    \n",
    "    - Products without categories, no name information, description, photos and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:30.556056Z",
     "start_time": "2019-08-27T20:10:30.544769Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f'Dataset: {name}\\n')\n",
    "    print(f'{df.dtypes}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in some attributes we have the wrong data type assigned. For example, there are date columns in orders dataset assigned as strings. Let's apply the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:30.89107Z",
     "start_time": "2019-08-27T20:10:30.558626Z"
    }
   },
   "outputs": [],
   "source": [
    "time_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "           'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "for col in time_cols:\n",
    "    olist_orders[col] = pd.to_datetime(olist_orders[col])\n",
    "    \n",
    "# Checking\n",
    "olist_orders.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a better relationship with the data and the business problem, let's propose an analysis using graphical tools. This session is really important for understanding brazilian e-commerce and how online shopping can influence on customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will go trough:\n",
    "\n",
    "* Evolution of Brazilian E-commerce Overtime\n",
    "* Online Purchasing on Brazilian States\n",
    "* Review Score Given by Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of E-Commerce Overtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that online shopping is growing up each day. In Brazil, this is no different. Few years ago, there was a kind of fear about buying things online. Nowadays, many people prefer the conveniance and facility that only e-commerce can offer.\n",
    "\n",
    "Does the data we have in hands show this perspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:31.755695Z",
     "start_time": "2019-08-27T20:10:30.893193Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Creating new year column\n",
    "olist_orders['order_purchase_year'] = \\\n",
    "olist_orders['order_purchase_timestamp'].apply(lambda x: x.year)\n",
    "\n",
    "# Creating new month column\n",
    "olist_orders['order_purchase_month'] = \\\n",
    "olist_orders['order_purchase_timestamp'].apply(lambda x: x.month)\n",
    "\n",
    "# Creating new day of week column\n",
    "olist_orders['order_purchase_dayofweek'] = \\\n",
    "olist_orders['order_purchase_timestamp'].apply(lambda x: x.dayofweek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Online Orders by Year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:31.997537Z",
     "start_time": "2019-08-27T20:10:31.757594Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Preparing the chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = sns.countplot(x='order_purchase_year', data=olist_orders, palette='Blues_d')\n",
    "format_spines(ax, right_border=False)\n",
    "\n",
    "# Showing frequency\n",
    "ncount = len(olist_orders)\n",
    "for p in ax.patches:\n",
    "    x=p.get_bbox().get_points()[:,0]\n",
    "    y=p.get_bbox().get_points()[1,1]\n",
    "    ax.annotate('{:.1f}%'.format(100.*y/ncount), (x.mean(), y), \n",
    "            ha='center', va='bottom', size=12)\n",
    "\n",
    "ax.set_title('Amount of Online Order by Year', size=14)\n",
    "ax.set_ylabel('Orders')\n",
    "ax.set_xlabel('Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a rising trend but we have to be carefull about the extremelly small value in 2016. Maybe our data starts in the middle of that year so the counting of orders would be really small. Let's see the range of dates of orders dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:32.004998Z",
     "start_time": "2019-08-27T20:10:31.999775Z"
    }
   },
   "outputs": [],
   "source": [
    "min_order_date = olist_orders['order_purchase_timestamp'].min()\n",
    "max_order_date = olist_orders['order_purchase_timestamp'].max()\n",
    "print(f'We have orders from {min_order_date} to {max_order_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we thought, in 2016 the order register starts from September. It's not fair to say that online orders grew up from 0.3% in that year for 45.4% next year. Let's explore this in a vision that includes the sales amount by month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Total Sales Overtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:32.261412Z",
     "start_time": "2019-08-27T20:10:32.01013Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Joining order and items datasets\n",
    "df_orders_items = olist_orders.merge(olist_order_items, on='order_id', how='inner')\n",
    "\n",
    "# Creating a column for calculating item + freight value\n",
    "df_orders_items['total_sales'] = df_orders_items['price'] + df_orders_items['freight_value']\n",
    "\n",
    "# Grouping the dataset by month-year sales\n",
    "df_sales = df_orders_items.groupby(['order_purchase_year', 'order_purchase_month'], \n",
    "                         as_index=False).sum()\n",
    "df_sales = df_sales.loc[:, ['order_purchase_year', 'order_purchase_month', 'total_sales']]\n",
    "\n",
    "# Splitting sets by year\n",
    "df_sales_2016 = df_sales[df_sales['order_purchase_year']==2016]\n",
    "df_sales_2017 = df_sales[df_sales['order_purchase_year']==2017]\n",
    "df_sales_2018 = df_sales[df_sales['order_purchase_year']==2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:32.314815Z",
     "start_time": "2019-08-27T20:10:32.263759Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Filling months that are not showing on datasets\n",
    "months = np.arange(1, 13)\n",
    "df_append = pd.DataFrame({})\n",
    "grouped_dfs = df_sales_2016, df_sales_2017, df_sales_2018\n",
    "years = [2016, 2017, 2018]\n",
    "idx = 0\n",
    "\n",
    "for df in grouped_dfs:\n",
    "    for month in months:\n",
    "        # If there is a nonexistent month\n",
    "        if month not in df['order_purchase_month'].values:\n",
    "            dict_append = {\n",
    "                'order_purchase_year': years[idx],\n",
    "                'order_purchase_month': month,\n",
    "                'total_sales': 0\n",
    "            }\n",
    "            df_append = df_append.append(dict_append, ignore_index=True)\n",
    "    # Insert the dictionary on dataset and sort it by month\n",
    "    df = df.append(df_append).astype(int)\n",
    "    df.sort_values(by='order_purchase_month', inplace=True)\n",
    "    df_append = pd.DataFrame({})\n",
    "    # Index the result on each dataset\n",
    "    if idx == 0:\n",
    "        df_sales_2016 = df\n",
    "    elif idx == 1:\n",
    "        df_sales_2017 = df\n",
    "    else:\n",
    "        df_sales_2018 = df\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:33.13579Z",
     "start_time": "2019-08-27T20:10:32.316428Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Total sales by month\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 12))\n",
    "bar_plot(x='order_purchase_month', y='total_sales', df=df_sales_2016, ax=axs[0], value=True)\n",
    "bar_plot(x='order_purchase_month', y='total_sales', df=df_sales_2017, ax=axs[1], value=True)\n",
    "bar_plot(x='order_purchase_month', y='total_sales', df=df_sales_2018, ax=axs[2], value=True)\n",
    "\n",
    "# Customizing\n",
    "axs_list = axs[0], axs[1], axs[2]\n",
    "for ax in axs_list:\n",
    "    format_spines(ax, right_border=False)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Total Sales (R$)')\n",
    "axs[0].set_title('Monthly Sales in 2016', size=14)\n",
    "axs[1].set_title('Monthly Sales in 2017', size=14)\n",
    "axs[2].set_title('Monthly Sales in 2018', size=14, pad=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sales Overtime Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:34.020958Z",
     "start_time": "2019-08-27T20:10:33.137766Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(13, 10))\n",
    "\n",
    "# Axis definition\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[1, :])\n",
    "ax2 = fig.add_subplot(gs[0, 0])\n",
    "ax3 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "# --------- EVOLUTION OF ONLINE SALES ------------\n",
    "\n",
    "# Creating a year-month column for correct sorting the data\n",
    "df_sales['order_purchase_month'] = df_sales['order_purchase_month'].astype(str).\\\n",
    "apply(lambda x: '0' + x if len(x) == 1 else x)\n",
    "df_sales['month_year'] = df_sales['order_purchase_year'].astype(str) + '-' + \\\n",
    "df_sales['order_purchase_month'].astype(str)\n",
    "\n",
    "# Changing the datatype of this new column\n",
    "df_sales['order_purchase_month'] = df_sales['order_purchase_month'].astype(int)\n",
    "\n",
    "# Plotting\n",
    "sns.lineplot(x='month_year', y='total_sales', data=df_sales.iloc[:-1, :], ax=ax1)\n",
    "format_spines(ax1, right_border=False)\n",
    "ax1.tick_params(axis='x', labelrotation=90)\n",
    "ax1.set_xlabel('Time', labelpad=20)\n",
    "ax1.set_ylabel('Total Sales (R$)')\n",
    "ax1.set_title('Evolution of Online Sales', size=14)\n",
    "\n",
    "\n",
    "# --------- SALES EVOLUTION OVER THE LAST TWO YEARS ------------\n",
    "\n",
    "# Crossing data from 2017 and 2018\n",
    "sns.lineplot(x='order_purchase_month', y='total_sales', data=df_sales_2017, label='2017', ax=ax2)\n",
    "sns.lineplot(x='order_purchase_month', y='total_sales', \n",
    "                  data=df_sales_2018.iloc[:-4, :], label='2018', ax=ax2)\n",
    "format_spines(ax2, right_border=False)\n",
    "ax2.set_xlabel('Time', labelpad=15)\n",
    "ax2.set_ylabel('Total Sales (R$)')\n",
    "ax2.set_title('Sales Evolution Over the Last Two Years', size=14)\n",
    "ax2.set_xticks(np.arange(13))\n",
    "ax2.set_xticklabels(['', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', \n",
    "                           'Oct', 'Nov', 'Dec'])\n",
    "\n",
    "# --------- AVERAGE SALES BY DAY OF WEEK ------------\n",
    "\n",
    "# Average sales by day of week\n",
    "df_sales_dow_mean = \\\n",
    "df_orders_items.groupby(['order_purchase_year', 'order_purchase_dayofweek'], \n",
    "                        as_index=False).mean().iloc[:, np.c_[0, 1, 6][0]]\n",
    "\n",
    "# Plotting\n",
    "bar_plot(x='order_purchase_dayofweek', y='total_sales', ax=ax3,\n",
    "         df=df_sales_dow_mean, colors='YlGnBu', value=True)\n",
    "ax3.set_xlabel('Day of Week', labelpad=15)\n",
    "ax3.set_ylabel('Average Sales (R$)')\n",
    "ax3.set_title('Average Sales by Day of Week', size=14)\n",
    "format_spines(ax3, right_border=False)\n",
    "ax3.set_xticks(np.arange(7))\n",
    "ax3.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above we can see that online sales are really increasing overtime! E-commerce is shinning in Brazil! The analysis by day of week shows that in general the average of sales is higher on Saturdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Sales by Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding the behavior of e-commerce in Brazil, let's take a look at this evolution by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:34.623182Z",
     "start_time": "2019-08-27T20:10:34.022943Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Connecting customers dataset\n",
    "df_sales_customer = df_orders_items.merge(olist_customer, on='customer_id', how='inner')\n",
    "\n",
    "# Grouping sales by state\n",
    "df_sales_state = df_sales_customer.groupby(['customer_state'], \n",
    "                                          as_index=False).sum().iloc[:, np.c_[(0, -2, -3)][0]]\n",
    "df_sales_state.sort_values(by='total_sales', ascending=False, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 10))\n",
    "sns.barplot(x='total_sales', y='customer_state', data=df_sales_state, ci=None,\n",
    "                 palette='Blues_d')\n",
    "format_spines(ax, right_border=False)\n",
    "ax.set_title('Total Sales by State', size=14)\n",
    "ax.set_xlabel('Total Sales (R$)')\n",
    "ax.set_ylabel('State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São Paulo, Rio de Janeiro and Minas Gerais are the top 3 states with the highest amount sold online. These three states are located on brazilian southeast region. Let's see total sales in a map using the `folium` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:34.853763Z",
     "start_time": "2019-08-27T20:10:34.625107Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/luizpedone/municipal-brazilian-geodata/master/data'\n",
    "world_geo = f'{url}/Brasil.json'\n",
    "json_data = gpd.read_file(f'{url}/Brasil.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:35.203674Z",
     "start_time": "2019-08-27T20:10:34.855511Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "brazilian_data = json_data.merge(df_sales_state, left_on='UF', right_on='customer_state', how='left').fillna(0)\n",
    "brazilian_data['total_sales_plot'] = brazilian_data['total_sales'].apply(lambda x: round(x/1000, 1)).astype(str)\n",
    "brazilian_data['total_sales_plot'] = brazilian_data['total_sales_plot'].apply(lambda x: x + 'k')\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[-16.5, -52.6], \n",
    "    zoom_start=4,\n",
    "    tiles='stamenwatercolor'\n",
    ")\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=json_data,\n",
    "    name='Online Sales',\n",
    "    data=df_sales_state,\n",
    "    columns=['customer_state', 'total_sales'],\n",
    "    key_on='feature.properties.UF',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    nan_fill_color='white',\n",
    "    nan_fill_opacity=0.9,\n",
    "    legend_name='Online Sales',\n",
    ").add_to(m)\n",
    "\n",
    "Fullscreen(\n",
    "    position='topright',\n",
    "    title='Expand me',\n",
    "    title_cancel='Exit me',\n",
    "    force_separate_button=True\n",
    ").add_to(m)\n",
    "\n",
    "folium.GeoJson(\n",
    "    brazilian_data,\n",
    "    style_function=style_function,\n",
    "    highlight_function=highlight_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['ESTADO', 'total_sales_plot'],\n",
    "                                  aliases=['State:', 'Total Sales:'],\n",
    "                                  labels=True,\n",
    "                                  sticky=True)\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we can see in the map that the three states mentioned above (Sao Paulo, Rio and Minas Gerais) are really standing out from the others. E-commerce is really strong on them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next graphical report, we will bring up an analysis on brazilian states considering:\n",
    "\n",
    "* **Freight value**\n",
    "* **Mean delivery time**\n",
    "* **Difference between delivery time and estimated time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:36.534903Z",
     "start_time": "2019-08-27T20:10:35.207377Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Delivery working days\n",
    "df_delivery_time = add_series_working_days(series_name='time_to_delivery', \n",
    "                                           df=df_sales_customer,\n",
    "                                           date_col1 = 'order_purchase_timestamp', \n",
    "                                           date_col2 = 'order_delivered_customer_date')\n",
    "\n",
    "# Grouping by delivery time and state\n",
    "df_delivery_time = df_delivery_time.groupby(['customer_state'], as_index=False).mean()\n",
    "df_delivery_time = df_delivery_time.iloc[:, np.c_[(0, -1)][0]]\n",
    "\n",
    "# Working days between delivery time and estimated time\n",
    "df_diff_est_deliv = add_series_working_days(series_name='diff_est_deliv', \n",
    "                                           df=df_sales_customer,\n",
    "                                           date_col1 = 'order_delivered_customer_date', \n",
    "                                           date_col2 = 'order_estimated_delivery_date')\n",
    "\n",
    "# Grouping by state \n",
    "df_diff_est_deliv = df_diff_est_deliv.groupby(['customer_state'], as_index=False).mean()\n",
    "df_diff_est_deliv = df_diff_est_deliv.iloc[:, np.c_[(0, -1)][0]]\n",
    "df_diff_est_deliv['diff_est_deliv'] = df_diff_est_deliv['diff_est_deliv'].astype(int)\n",
    "\n",
    "# Calculating parameters\n",
    "df_mean_freight = df_sales_customer.groupby(['customer_state'], \n",
    "                                as_index=False).mean().iloc[:, np.c_[(0, -2, -3)][0]]\n",
    "mean_freight_value = df_mean_freight['freight_value'].mean()\n",
    "mean_delivery_time = df_delivery_time['time_to_delivery'].mean()\n",
    "mean_diff_estimative = df_diff_est_deliv['diff_est_deliv'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:37.406116Z",
     "start_time": "2019-08-27T20:10:36.536305Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axs = plt.subplots(3, 3, figsize=(13, 8))\n",
    "communicate_params(mean_freight_value, mean_delivery_time, mean_diff_estimative)\n",
    "\n",
    "# Reight information\n",
    "plot_param(df=df_mean_freight, col='freight_value', \n",
    "           title='Top 5 States with \\nHighest Freight Value', xlim=45, n_row=1, n_col=0, xlabel='Mean Freight')\n",
    "\n",
    "# Delivery time information\n",
    "plot_param(df=df_delivery_time, col='time_to_delivery', \n",
    "           title='Top 5 States with \\nHighest Delivery Time', xlim=21, n_row=1, n_col=1, \n",
    "           xlabel='Mean Delivery Time')\n",
    "\n",
    "# Difference between estimative delivery and real delivery time\n",
    "plot_param(df=df_diff_est_deliv, col='diff_est_deliv', \n",
    "           title='Top 5 States with Highest Difference \\nBetween Delivery and Estimated Time', \n",
    "           xlim=15, n_row=1, n_col=2, xlabel='Diff Estimated and Delivery')\n",
    "\n",
    "plt.suptitle('Comparative Study: E-Commerce on Brazilian States', size=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payment Type on E-Commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will see how brazilian people usually pay online orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Payment Type and Evolution Overtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:37.734126Z",
     "start_time": "2019-08-27T20:10:37.410248Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Setting up dataset\n",
    "df_orders_payments = olist_orders.merge(olist_order_payments, on='order_id', how='left')\n",
    "payments_evolution = df_orders_payments.groupby(by=['order_purchase_year', 'order_purchase_month',\n",
    "                                                    'payment_type'], as_index=False).count().iloc[:, :4]\n",
    "\n",
    "payments_evolution = payments_evolution.query('payment_type != \"not_defined\"')\n",
    "\n",
    "# Transforming month column for correct sorting\n",
    "payments_evolution['order_purchase_month'] = payments_evolution['order_purchase_month'].astype(str).\\\n",
    "apply(lambda x: '0' + x if len(x) == 1 else x)\n",
    "\n",
    "# Creating new column year-month\n",
    "payments_evolution['month_year'] = payments_evolution['order_purchase_year'].astype(str) + '-' + \\\n",
    "payments_evolution['order_purchase_month'].astype(str)\n",
    "\n",
    "# Correcting data type\n",
    "payments_evolution['order_purchase_month'] = payments_evolution['order_purchase_month'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.680207Z",
     "start_time": "2019-08-27T20:10:37.736702Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(10, 10))\n",
    "\n",
    "# Defining axis\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "# Total payments\n",
    "total_transacoes = len(olist_order_payments)\n",
    "total_pagamentos = olist_order_payments['payment_value'].sum()\n",
    "ax1.text(0.12, 0.65, f'{str(total_transacoes)[:3]}.', fontsize=30, color='navy')\n",
    "ax1.text(0.33, 0.65, f'{str(total_transacoes)[3:]}', fontsize=30, color='navy')\n",
    "ax1.text(0.00, 0.57, 'is the total payments registered', fontsize=12)\n",
    "ax1.text(0.00, 0.37, f'R${str(total_pagamentos)[:2]}.', fontsize=25, color='navy')\n",
    "ax1.text(0.23, 0.37, f'{str(total_pagamentos)[2:5]}.', fontsize=25, color='navy')\n",
    "ax1.text(0.40, 0.37, f'{str(total_pagamentos)[5:8]},', fontsize=25, color='navy')\n",
    "ax1.text(0.57, 0.37, f'{str(total_pagamentos)[9:]}', fontsize=25, color='navy')\n",
    "ax1.text(0.05, 0.28, 'is the total amount registered', fontsize=12)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Payment type proportion\n",
    "payments_labels = olist_order_payments['payment_type'].value_counts().index\n",
    "colors = ['navy', 'steelblue', 'lightsteelblue', 'powderblue']\n",
    "donut_plot('payment_type', ax2, olist_order_payments, labels=payments_labels, text='', \n",
    "           colors=colors, flag_ruido=1)\n",
    "\n",
    "# Sales evolution by payment type\n",
    "sns.lineplot(x='month_year', y='order_id', data=payments_evolution, hue='payment_type', ax=ax3)\n",
    "format_spines(ax3, right_border=False)\n",
    "ax3.tick_params(axis='x', labelrotation=90)\n",
    "ax3.set_xlabel('Time', labelpad=20)\n",
    "ax3.set_ylabel('Transactions')\n",
    "ax3.set_title('Evolution of Online Transactions by Payment Type', size=14)\n",
    "\n",
    "fig.suptitle(\"Brazilian E-Commerce Study by Payment Type\", size=16)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's really interesting to see the evolution of payments with credit card. This method is preferred among brazilian customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we went through a lot of good analysis in Brazilian E-Commerce! Great! Now we will plot the last chart from our Exploratory Data Analysis and this is a special one because it brings up the customer reviews for each order.\n",
    "\n",
    "This can be our first step to going deep into **_Natural Language Processing_** because the review score is connected with customer's review messages. Let's see how the score is distributed in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.78053Z",
     "start_time": "2019-08-27T20:10:38.681915Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "reviews_labels = olist_order_reviews['review_score'].value_counts().index\n",
    "total_reviews = len(olist_order_reviews)\n",
    "text = f'Total Reviews\\n\\n{total_reviews}'\n",
    "colors = ['navy', 'steelblue', 'lightsteelblue', 'powderblue', 'lightcyan']\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "donut_plot('review_score', ax, olist_order_reviews, labels=reviews_labels, text=text, colors=colors)\n",
    "ax.set_title('Reviews on Online Orders', size=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the EDA session, let's start the analysis of messages left from customers after buying something online. All messages are in portuguese so maybe this experience won't be as good as you expect (if you are not brazilian), but my goal here is to go through all the steps I judge important on any NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.79731Z",
     "start_time": "2019-08-27T20:10:38.782589Z"
    }
   },
   "outputs": [],
   "source": [
    "olist_order_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information from this dataset:\n",
    "\n",
    "* **review_id:** primary key for each review;\n",
    "* **order_id:** foreign key for connecting orders dataset;\n",
    "* **review_score:** score given by the customer for the order;\n",
    "* **review_comment_title:** comment title left by customer;\n",
    "* **review_comment_message:** message given by customer;\n",
    "* **review_creation_date:** message creation date;\n",
    "* **review_answer_timestamp:** answear date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the null data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.855552Z",
     "start_time": "2019-08-27T20:10:38.799318Z"
    }
   },
   "outputs": [],
   "source": [
    "olist_order_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.88113Z",
     "start_time": "2019-08-27T20:10:38.858196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing null data\n",
    "reviews = olist_order_reviews.dropna(subset=['review_comment_message'])\n",
    "print(f'There are {reviews.shape[0]} reviews on the dataset\\n')\n",
    "\n",
    "# Examples\n",
    "for i in range(5):\n",
    "    print(f'Review {i+1}: {np.random.choice(reviews[\"review_comment_message\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will give the first steps for building a model for text classification and sentimental analysis. The following session will include RegEx application for threating some non-desired elements at the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break Line and Carriage Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.887676Z",
     "start_time": "2019-08-27T20:10:38.882694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "criticas = list(reviews['review_comment_message'].values)\n",
    "criticas[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's possible to see the tags \\r (_carriage return_ code ASCII 10) and \\n (_new line_ code ASCII 13). With RegEx, we will search for those patterns on texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.983869Z",
     "start_time": "2019-08-27T20:10:38.889517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying RegEx for removing \\n and \\r from reviews\n",
    "criticas_temp = []\n",
    "criticas_pos_quebra = []\n",
    "for c in criticas:\n",
    "    c = re.sub(r'\\n', ' ', c)\n",
    "    criticas_temp.append(c)\n",
    "for c in criticas_temp:\n",
    "    c = re.sub(r'\\r', ' ', c)\n",
    "    criticas_pos_quebra.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:38.988111Z",
     "start_time": "2019-08-27T20:10:38.985054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the same review\n",
    "criticas_pos_quebra[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! We removed the tags and now we can go to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sites and Hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as we are talking about online reviews, it's possible to find anything on the comments. Sites and hiperlinks are very common and we must look if we have this on our reviews. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.066426Z",
     "start_time": "2019-08-27T20:10:38.989642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying RegEx for removing urls and sites\n",
    "criticas_pos_hyperlink = []\n",
    "for c in criticas_pos_quebra:\n",
    "    urls = re.findall('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', c)\n",
    "    if len(urls) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for url in urls:\n",
    "            for link in url:\n",
    "                c = c.replace(link, '')\n",
    "        c = c.replace(':', '')\n",
    "        c = c.replace('/', '')\n",
    "    criticas_pos_hyperlink.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.074448Z",
     "start_time": "2019-08-27T20:10:39.067643Z"
    }
   },
   "outputs": [],
   "source": [
    "exemplos = []\n",
    "idx1 = 10796\n",
    "idx2 = 12782\n",
    "exemplos.append((criticas_pos_quebra[idx1], criticas_pos_hyperlink[idx1]))\n",
    "exemplos.append((criticas_pos_quebra[idx2], criticas_pos_hyperlink[idx2]))\n",
    "i = 1\n",
    "for e in exemplos:\n",
    "    print(f'- - - COMMENT {i} - - -\\n')\n",
    "    print(f'Before RegEx: \\n{e[0]}\\n')\n",
    "    print(f'After RegEx: \\n{e[1]}\\n\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to find numbers on reviews and replace them with another string `numero` (that means `number`, in english). We could just replace the numbers with whitespace but maybe this would generated some information loss. Let's see what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.181565Z",
     "start_time": "2019-08-27T20:10:39.075725Z"
    }
   },
   "outputs": [],
   "source": [
    "criticas_pos_num = []\n",
    "for c in criticas_pos_hyperlink:\n",
    "    c = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'numero', c)\n",
    "    criticas_pos_num.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:22:09.378274Z",
     "start_time": "2019-08-27T20:22:09.368625Z"
    }
   },
   "outputs": [],
   "source": [
    "exemplos = []\n",
    "idx1 = 68\n",
    "exemplos.append((criticas_pos_quebra[idx1], criticas_pos_num[idx1]))\n",
    "i = 1\n",
    "for e in exemplos:\n",
    "    print(f'- - - COMMENT {i} - - -\\n')\n",
    "    print(f'Before RegEx: \\n{e[0]}\\n')\n",
    "    print(f'After RegEx: \\n{e[1]}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It worked. Keep going..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Characteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we finally came at this really important point: special characteres. In fact, this is very common online and certainly we need to deal with it in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.1972Z",
     "start_time": "2019-08-27T20:10:39.192665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "print(f'Review 45: {criticas_pos_num[45]}\\n')\n",
    "print(f'Review 135: {criticas_pos_num[135]}\\n')\n",
    "print(f'Review 234: {criticas_pos_num[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.420496Z",
     "start_time": "2019-08-27T20:10:39.19855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying RegEx and checking results\n",
    "criticas_sem_alphanumerico = []\n",
    "for c in criticas_pos_num:\n",
    "    c = re.sub(r'R\\$', ' ', c)\n",
    "    c = re.sub(r'\\W', ' ', c)\n",
    "    criticas_sem_alphanumerico.append(c)\n",
    "    \n",
    "print(f'Crítica 45: {criticas_sem_alphanumerico[45]}\\n')\n",
    "print(f'Crítica 135: {criticas_sem_alphanumerico[135]}\\n')\n",
    "print(f'Crítica 234: {criticas_sem_alphanumerico[234]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We removed money symbols (R$) and special characteres from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have something interesting: after all our transformation, maybe we have left behind some \"noise\" on the corpus. As long as we have been applying RegEx, we replaced almost everything with space, so probably our corpus has a lot of them. Also, maybe people who write online comments make mistakes too, so we have deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.429131Z",
     "start_time": "2019-08-27T20:10:39.423953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "print(f'Review 3: {criticas_sem_alphanumerico[3]}\\n')\n",
    "print(f'Review 48: {criticas_sem_alphanumerico[48]}\\n')\n",
    "print(f'Review 51: {criticas_sem_alphanumerico[51]}\\n')\n",
    "print(f'Review 2569: {criticas_sem_alphanumerico[2569]}\\n')\n",
    "print(f'Review 3856: {criticas_sem_alphanumerico[3856]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.702149Z",
     "start_time": "2019-08-27T20:10:39.430882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying RegEx and checking results\n",
    "criticas_sem_espacos_adicionais = []\n",
    "for c in criticas_sem_alphanumerico:\n",
    "    c = re.sub(r'\\s+', ' ', c)\n",
    "    criticas_sem_espacos_adicionais.append(c)\n",
    "    \n",
    "print(f'Review 3: {criticas_sem_espacos_adicionais[3]}\\n')\n",
    "print(f'Review 48: {criticas_sem_espacos_adicionais[48]}\\n')\n",
    "print(f'Review 51: {criticas_sem_espacos_adicionais[51]}\\n')\n",
    "print(f'Review 2569: {criticas_sem_espacos_adicionais[2569]}\\n')\n",
    "print(f'Review 3856: {criticas_sem_espacos_adicionais[3856]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.755317Z",
     "start_time": "2019-08-27T20:10:39.704615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the progress\n",
    "processed_reviews = reviews.copy()\n",
    "processed_reviews['review_after_regex'] = criticas_sem_espacos_adicionais\n",
    "processed_reviews = processed_reviews.iloc[:, np.r_[4, -1, 2]]\n",
    "processed_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, the stopwords will be removed with lib nltk. The punctuations will be removed too. Let's see how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.769451Z",
     "start_time": "2019-08-27T20:10:39.758786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "after_regex = list(processed_reviews['review_after_regex'].values)\n",
    "c = after_regex[462]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:10:39.78916Z",
     "start_time": "2019-08-27T20:10:39.771327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying function for text processing\n",
    "c_processed = text_process(c)\n",
    "c_processed = ' '.join(c_processed)\n",
    "c_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:11:38.474172Z",
     "start_time": "2019-08-27T20:10:39.791303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying transformations on the entire dataset\n",
    "processed_reviews['review_after_stopwords'] = processed_reviews['review_after_regex'].apply(text_process)\n",
    "processed_reviews['review_after_stopwords'] = processed_reviews['review_after_stopwords'].apply(lambda x: ' '.join(x))\n",
    "processed_reviews['review_after_stopwords'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply stemming using `RSLPStemmer()` function. Maybe this can be a little but confuse, specially if you're not from Brazil, but think of it as a process of reducing inflected words to their word stem. We will see some examples in portuguese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:11:38.485604Z",
     "start_time": "2019-08-27T20:11:38.475597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "exemplo1 = processed_reviews['review_after_stopwords'][15]\n",
    "exemplo2 = list(processed_reviews['review_after_stopwords'])[3856]\n",
    "\n",
    "print(f'Review 15: {exemplo1}\\n')\n",
    "print(f'Review 3856: {exemplo2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:12:26.1805Z",
     "start_time": "2019-08-27T20:11:38.486927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying stemming\n",
    "processed_reviews['review_after_stemming'] = processed_reviews['review_after_regex'].apply(stem_processing)\n",
    "processed_reviews['review_after_stemming'] = processed_reviews['review_after_stemming'].apply(lambda x: ' '.join(x))\n",
    "processed_reviews['review_after_stemming'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the steps above, we can finally say that we're almost there! Here I prove this to you by creating our sentiment label based on `review_score` attribute. Basically we will use the rating given by the customer on online shopping to flag the review as positive or negative, following the range below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Score 1 and 2:** Negative (0)\n",
    "* **Score 3, 4 and 5:** Positive (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this and keep moving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:19:37.96806Z",
     "start_time": "2019-08-27T20:19:37.926254Z"
    }
   },
   "outputs": [],
   "source": [
    "bin_edges = [0, 2, 5]\n",
    "bin_names = ['0', '1']\n",
    "processed_reviews['class'] = pd.cut(processed_reviews['review_score'], bins=bin_edges, labels=bin_names)\n",
    "processed_reviews = processed_reviews.iloc[:, np.r_[0, 1, 3, 4, 2, 5]]\n",
    "processed_reviews.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I show you all the progress we've made so far. To the first column (review_comment_message) to the fourth one (review_after_stemming) we have the result of each step we applied for text preparation. Fantastic! But we have to do one more thing: transforming the words into vectors so the Machine Learning algorithm can understand what's going on. For this we will use **_TF-IDF_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the data to put it into a Machine Learning algorithm so the model can properly make assumptions from text reviews, it's necessary to transform it into numbers. But how to do that? There is different approaches, for example, _Bag of Words_, _TF-IDF_ and _Word2Vec_.\n",
    "\n",
    "In the _Bag of Words_ approach, we create a vocabulary with all unique words and, for each review, the vector number means simply the frequency of that word on the corpus. On this way, each word have the same weight, wich probably wouldn't be so nice, mainly if we are talking about the low occurrence words.\n",
    "\n",
    "So, _TF-IDF_ approach (Term Frequency and Inverse Document Frequency) can be used for deal with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    TF=\\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    IDF = \\log\\left({\\frac{\\text{Total number of docs}}{\\text{Number of docs containing the words}}}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:27:18.80365Z",
     "start_time": "2019-08-27T20:27:18.373996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying TD-IDF\n",
    "X = list(processed_reviews['review_after_stemming'])\n",
    "y = processed_reviews['class'].values\n",
    "y = y.astype(int)\n",
    "count_vectorizer = CountVectorizer(max_features=300, stop_words=stopwords.words('portuguese')).fit(X)\n",
    "vectorizer = TfidfVectorizer(max_features=300, stop_words=stopwords.words('portuguese'))\n",
    "processed_features = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Review processed\n",
    "processed_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters given for `CountVectorizer` class are:\n",
    "\n",
    "* **max_features=300**, which means that it only uses the 300 most frequently occurring words to create a bag of words feature vector. Words that occur less frequently are not very useful for classification.\n",
    "* **min_df=7**, which shows that include words that occur in at least 7 documents\n",
    "* **max_df=0.8**, that specifies that only use those words that occur in a maximum of 80% of the documents. Words that occur in all documents are too common and are not very useful for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF from words at first review\n",
    "for i in processed_features[0]:\n",
    "    if i > 0:\n",
    "        print(f'Word on Vocab: {list(processed_features[0]).index(i)} - TF-IDF: {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding, let's see some words for our vocabulary built from TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k, v in count_vectorizer.vocabulary_.items():\n",
    "    tfidf = vectorizer.idf_[count_vectorizer.vocabulary_[k]]\n",
    "    print(f'Palavra: {k:<12} - Contagem: {v:<10} - TF-IDF: {tfidf}')\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the game really starts...\n",
    "\n",
    "Let's use the real power of sklearn to create a Pipeline for all the preparation steps we've made until here. I have to tell you that this will be sound like magic and you will see how beautiful it's gonna be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create Python classes for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:31:44.956591Z",
     "start_time": "2019-08-27T20:31:44.947067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class for applying Regular Expressions\n",
    "class ApplyRegex(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, break_line=True, carriage_return=True, numbers=True, number_replacing='number', \n",
    "                 special_char=True, additional_spaces=True):\n",
    "        self.break_line = break_line\n",
    "        self.carriage_return = carriage_return\n",
    "        self.numbers = numbers\n",
    "        self.number_replacing = number_replacing\n",
    "        self.special_char = special_char\n",
    "        self.additional_spaces = additional_spaces\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for c in X:\n",
    "            if self.break_line:\n",
    "                c = re.sub('\\n', ' ', c)\n",
    "            if self.carriage_return:\n",
    "                c = re.sub('\\r', ' ', c)\n",
    "            if self.numbers:\n",
    "                c = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' '+self.number_replacing+' ', c)\n",
    "            if self.special_char:\n",
    "                c = re.sub(r'R\\$', ' ', c)\n",
    "                c = re.sub(r'\\W', ' ', c)\n",
    "            if self.additional_spaces:\n",
    "                c = re.sub(r'\\s+', ' ', c)\n",
    "            X_transformed.append(c)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:32:01.011124Z",
     "start_time": "2019-08-27T20:32:01.0026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class for removing punctuation and Stopwords\n",
    "class StopWordsPunctuationRemoval(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = list(map(lambda c: text_process(c), X))\n",
    "        X_transformed = list(map(lambda x: ' '.join(x), X_transformed))\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:32:23.575335Z",
     "start_time": "2019-08-27T20:32:23.5698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class for applying Stemming\n",
    "class TextStemming(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"X_Series = pd.Series(X)\n",
    "        X_transformed = X_Series.apply(stem_processing)\n",
    "        X_transformed = X_transformed.apply(lambda x: ' '.join(x))\n",
    "        return list(X_transformed.values)\"\"\"\n",
    "        X_transformed = list(map(lambda c: stem_processing(c), X))\n",
    "        X_transformed = list(map(lambda x: ' '.join(x), X_transformed))\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:32:58.777977Z",
     "start_time": "2019-08-27T20:32:58.770177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a complete pre processing pipeline\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('regex_cleaner', ApplyRegex()),\n",
    "    ('stopwords_punc_remover', StopWordsPunctuationRemoval()),\n",
    "    ('stemming', TextStemming()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we have just received the original data and we want to apply all the pre processing steps in one cell. Is that even possible? Yes! Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:36:56.641437Z",
     "start_time": "2019-08-27T20:35:38.492786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the class label\n",
    "bin_edges = [0, 2, 5]\n",
    "bin_names = ['0', '1']\n",
    "reviews['class'] = pd.cut(reviews['review_score'], bins=bin_edges, labels=bin_names)\n",
    "X = reviews['review_comment_message']\n",
    "y = reviews['class'].values\n",
    "y = y.astype(int)\n",
    "\n",
    "# Applying pipeline\n",
    "X_preprocessed = preprocess_pipeline.fit_transform(X)\n",
    "\n",
    "# Creating a vocabulary with Count Vectorizer\n",
    "vectorizer = CountVectorizer(max_features=300, stop_words=stopwords.words('portuguese'))\n",
    "X_transformed = vectorizer.fit_transform(X_preprocessed).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:37:25.049129Z",
     "start_time": "2019-08-27T20:37:25.040076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking\n",
    "X_transformed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see!? We are now **ready to train a classification model**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train a model to do sentimental analysis on reviews. Let's split the data before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:45:21.416594Z",
     "start_time": "2019-08-27T20:45:21.357219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=.20, random_state=42)\n",
    "\n",
    "# Dimensions\n",
    "print(f'Dimensões X_train: {X_train.shape}')\n",
    "print(f'Dimensões y_train: {y_train.shape}\\n')\n",
    "print(f'Dimensões X_test: {X_test.shape}')\n",
    "print(f'Dimensões y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:45:30.93924Z",
     "start_time": "2019-08-27T20:45:25.448036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training a Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating results\n",
    "empty_train_performance = create_dataset()\n",
    "log_reg_performance = model_analysis(log_reg, X_train, y_train, \n",
    "                                     X_test, y_test, empty_train_performance)\n",
    "log_reg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:45:58.794761Z",
     "start_time": "2019-08-27T20:45:57.332514Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "cv_predictions = cross_val_predict(log_reg, X_train, y_train, cv=5)\n",
    "test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cv_cm = confusion_matrix(y_train, cv_predictions)\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "classes = ['Negative', 'Positive']\n",
    "plt.subplot(121)\n",
    "plot_confusion_matrix(cv_cm, classes, title='Logistic Regression\\nCV Confusion Matrix')\n",
    "plt.subplot(122)\n",
    "plot_confusion_matrix(test_cm, classes, cmap=plt.cm.Reds, title='Logistic Regression\\nTest Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:47:02.96335Z",
     "start_time": "2019-08-27T20:46:58.320399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "empty_train_performance = create_dataset()\n",
    "nb_performance = model_analysis(nb, X_train, y_train, \n",
    "                                X_test, y_test, empty_train_performance)\n",
    "nb_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T20:58:51.850724Z",
     "start_time": "2019-08-27T20:58:50.530264Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "cv_predictions = cross_val_predict(nb, X_train, y_train, cv=5)\n",
    "test_pred = nb.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cv_cm = confusion_matrix(y_train, cv_predictions)\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "classes = ['Negative', 'Positive']\n",
    "plt.subplot(121)\n",
    "plot_confusion_matrix(cv_cm, classes, title='Naive Bayes\\nCV Confusion Matrix')\n",
    "plt.subplot(122)\n",
    "plot_confusion_matrix(test_cm, classes, cmap=plt.cm.Reds, title='Naive Bayes\\nTest Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T21:02:02.404064Z",
     "start_time": "2019-08-27T21:02:02.302641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing new data\n",
    "frase = ['Péssimo produto! Nunca mais compro nessa loja, a entrega atrasou e custou muito dinheiro!']\n",
    "frase_preprocessed = preprocess_pipeline.fit_transform(frase)\n",
    "\n",
    "# Vectorizing\n",
    "frase_transformed = vectorizer.transform(frase_preprocessed).toarray()\n",
    "\n",
    "# Communicating\n",
    "plot_sentimento(log_reg, frase_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text example is similar to: _\"Terrible product! I will never buy at this store anymore, delivery was delayed and it cost a lot of money\"_\n",
    "\n",
    "And the model classified correctly as **Negative**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T21:04:35.314479Z",
     "start_time": "2019-08-27T21:04:35.223285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing new data\n",
    "frase2 = ['Muito bom! Adorei o produto e a loja forneceu um serviço excelente. Qualidade!']\n",
    "frase_preprocessed2 = preprocess_pipeline.fit_transform(frase2)\n",
    "\n",
    "# Vectorizing\n",
    "frase_transformed2 = vectorizer.transform(frase_preprocessed2).toarray()\n",
    "\n",
    "# Communicating\n",
    "plot_sentimento(log_reg, frase_transformed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing all the Natural Language Processing steps for classification and sentiment analysis among comments posted on e-commerce sites, it was possible to identify the importance of proper text processing and the effects caused by decisions made by the group.\n",
    "\n",
    "The preparation of the dataset undoubtedly required a thorough and specific analysis, requiring research, testing and validation of the most varied, always seeking to improve the results obtained and follow the best practices related to textual analysis. Concepts of Regular Expressions, Stemming, Stop Words removal and other unwanted words were applied. In addition, a final stage of preparation performed the procedure of transforming the corpus into sparse matrices, thus preparing the training of a classification model.\n",
    "\n",
    "So, two different models were tested: Logistic Regression and Gaussian Naive Bayes. The SVM model was also considered, but the high computational cost and processing time made this attempt impossible. After training, it is possible to consider the time required by each algorithm, being the Logistic Regression model the fastest and the best performing. Regarding the results, the group obtained the apex of f1_score with the Logistic Regression model.\n",
    "\n",
    "However, the performance of these models could be improved by better defining the sentiment of each comment, disengaging it from the critique score as this number often does not reflect the reality of the sentiment shown by the client. In addition, as the dataset comes from a relationship between user and interenet, some slang and characteristic abbreviations were present, making it difficult to understand the algorithm for proper classification. Searching for better hyperparameters with the GridSearchCV package could also help to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I hope you liked this kernel. Please upvote!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
